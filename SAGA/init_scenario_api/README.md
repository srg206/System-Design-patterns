# Init-Scenario-API


## Clean Architecture
Реализация (4 слоя)
1. Handler (API Layer) → internal/api/v1/init_scenario/
Принимает HTTP запросы, валидирует входные данные
Зависит от интерфейса InitScenarioUseCase, а не от конкретной реализации
2. UseCase (Business Logic) → internal/usecase/init_scenario/
Вся бизнес-логика (создание сценария + outbox в транзакции)
Зависит от интерфейса Repository, а не от PostgreSQL напрямую
3. Repository (Infrastructure) → internal/infastructure/repository/
Работа с БД, транзакции, SQL-запросы (через sqlc)
Реализует интерфейс из usecase

4. Models → internal/models/
dto - для API слоя
entity - для бизнес-логики
queries.Model - для БД


Почему именно так:  

✅ Dependency Inversion: Внутренние слои не знают о внешних    
✅ Тестируемость: Можно замокать любой слой через интерфейсы для тестов  
✅ Гибкость: Можно заменить PostgreSQL на MongoDB без изменения бизнес-логики  
✅ Изоляция: Бизнес-правила не зависят от HTTP/gRPC/Kafka

Проблемы при нарушении  
❌ Без интерфейсов: Handler → UseCase → *pgxpool.Pool  
Невозможно тестировать без реальной БД  
Бизнес-логика завязана на PostgreSQL  
❌ Без слоя UseCase: Handler → Repository напрямую  
Дублирование бизнес-логики в разных handlers  
Транзакционная логика размазана по API слою  
 



## Логгирование 

### Два подхода к организации логгирования

#### 1. Dependency Injection через Context (текущий подход)

**Как работает:**
- Логгер создается один раз при старте приложения
- Передается явно в middleware через конструктор
- Обогащается контекстной информацией (request_id, route)
- Вкладывается в `context.Context` и передается через все слои
- Извлекается в use case: `log := logger.FromContext(ctx)`

**Поток данных:**

---

### Сравнение подходов

| Критерий | **DI через Context (текущий)** | **Singleton** |
|----------|--------------------------------|---------------|
| **Тестируемость** | ✅ Легко мокать через context | ❌ Сложно мокать глобальную переменную |
| **Изоляция тестов** | ✅ Каждый тест может иметь свой logger | ❌ Все тесты используют один экземпляр |
| **Контекст запроса** | ✅ Автоматически передается (request_id, route) | ⚠️ Нужно вручную добавлять в каждый вызов |
| **Явность зависимостей** | ✅ Видно, что компонент использует логгер | ❌ Скрытая зависимость |
| **Простота использования** | ⚠️ Требует пробрасывать context | ✅ Вызов из любого места: `logger.GetLogger()` |
| **Производительность** | ⚠️ Overhead на context.Value() (~20ns) | ✅ Прямой доступ к переменной |
| **Goroutine safety** | ✅ Каждая горутина получает свой context | ✅ sync.Once + zap потокобезопасен |
| **Clean Architecture** | ✅ Соответствует принципам DI | ❌ Нарушает Dependency Inversion |
| **Гибкость** | ✅ Можно подменить логгер для части запросов | ❌ Один логгер на всё приложение |

---

### Способы сбора логов

#### 1. Agent-based (текущий подход - Promtail)

**Как работает:**
- Агент устанавливается на каждом узле/контейнере
- Читает логи из stdout/stderr или файлов
- Отправляет в централизованное хранилище (Loki, Elasticsearch)

**Примеры:** Promtail, Fluentd, Filebeat, Vector

**Плюсы:**
- Приложение не зависит от системы логирования
- Минимальный overhead на приложение
- Централизованная конфигурация агента

**Минусы:**
- Дополнительный компонент для деплоя
- Задержка в доставке логов
- Потребление ресурсов на агент

---

#### 2. Direct Shipping (прямая отправка)

**Как работает:**
- Приложение напрямую отправляет логи в систему хранения
- Используется SDK/библиотека (например, HTTP client к Loki API)

**Плюсы:**
- Нет промежуточного звена
- Минимальная задержка
- Меньше компонентов инфраструктуры

**Минусы:**
- Приложение зависит от доступности системы логирования
- Overhead на сетевые запросы
- Сложнее переключиться на другую систему

---

#### 3. Sidecar Pattern (Kubernetes)

**Как работает:**
- Отдельный контейнер в том же Pod'е читает логи
- Основное приложение пишет в stdout/файл
- Sidecar отправляет логи в систему хранения

**Плюсы:**
- Изоляция логики сбора от приложения
- Гибкая конфигурация на уровне Pod'а
- Легко менять логику сбора без пересборки приложения

**Минусы:**
- Дополнительные ресурсы на каждый Pod
- Сложнее управление

---

#### 4. Log Aggregation via Message Queue

**Как работает:**
- Логи отправляются в очередь (Kafka, RabbitMQ)
- Отдельный сервис читает из очереди и пишет в хранилище

**Плюсы:**
- Гарантированная доставка (persistent queue)
- Буферизация при недоступности хранилища
- Можно обрабатывать/фильтровать логи

**Минусы:**
- Сложная инфраструктура
- Overhead на message broker
- Задержка в доставке

---

### Выбор подхода

| Критерий | Agent-based | Direct Shipping | Sidecar | Message Queue |
|----------|-------------|-----------------|---------|---------------|
| **Простота** | ✅ | ✅ | ⚠️ | ❌ |
| **Производительность** | ✅ | ⚠️ | ✅ | ⚠️ |
| **Надежность** | ⚠️ | ❌ | ⚠️ | ✅ |
| **Изоляция** | ✅ | ❌ | ✅ | ✅ |
| **Инфраструктура** | ⚠️ | ✅ | ⚠️ | ❌ |

**В данном проекте используется Agent-based (Promtail):**
- Приложение независимо от системы логирования (можно заменить Loki на Elasticsearch)
- Простой деплой через Docker Compose
- Минимальный код в приложении
- Легко масштабировать


## Graceful Shutdown


Как реализуется
1) Ловим сигнал ОС (например SIGTERM, SIGINT). 
2) Останавливаем приём новых запросов / закрываем слушатели. 
3) Ждём завершения “in-flight” операций (с таймаутом). 
4) Закрываем/освобождаем ресурсы: соединения, файлы, транзакции.

Проблемы при остутсви graceful shutdown
1) Открытые транзакции / соединения останутся “висящими” → возможна неконсистентность данных, блокировки.
2) В окружениях с балансировкой/кластером (например Kubernetes) экземпляр может продолжать получать трафик, пока он уже “умирает”.

В нашем приложение graceful shutdown реалиуется с помощью паттерна Closer 

main() запускается  
↓  
[Инициализация App]  
├─ Closer.Add(logger.Sync) ← #1  
├─ Closer.Add(database.Close) ← #2  
└─ Closer.Add(srv.Shutdown) ← #3  
↓  
[HTTP сервер стартует в горутине]  
↓  
Closer.Wait() блокируется  
↓  
[ПОЛУЧЕН СИГНАЛ SIGTERM/SIGINT] ← Этап 1  
↓  
Closer.Close() запускается с таймаутом 30s  
↓  
Закрываются ресурсы в обратном порядке (LIFO):  
├─ #3 srv.Shutdown() ← Этап 2 + 3  
│   (останавливает приём новых запросов,  
│    ждет завершения текущих до 10s)  
├─ #2 database.Close() ← Этап 4  
│   (закрывает pool соединений)  
└─ #1 logger.Sync() ← Этап 4  
    (сбрасывает буферы логов)  
↓  
[Завершение программы]